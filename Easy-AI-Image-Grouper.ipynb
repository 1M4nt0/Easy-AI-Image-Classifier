{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyNJ8sqpk11FS6JLqJ5IErZ6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"06dff03f04f44623aaa0d90f8fc7b094":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abff852a6d664145b0ade5623e3c2429","IPY_MODEL_39702af6e3954da4a058d5bdd93b9459","IPY_MODEL_b0197fabd1334a90bf5a53efcc7ff943"],"layout":"IPY_MODEL_6148fb4a47414152837bc4946d2acf64"}},"abff852a6d664145b0ade5623e3c2429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e465e2f76f4eaeb34c411dfbc0ccc3","placeholder":"​","style":"IPY_MODEL_3bbc3e0bf8214e379b5d621b9d5884f8","value":"Epoch 1/30, Batch 4/15:  27%"}},"39702af6e3954da4a058d5bdd93b9459":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf27d162387641b887094c671290c844","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed3a12fecc5f4b8eba58844c7c64e694","value":4}},"b0197fabd1334a90bf5a53efcc7ff943":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_692b876d136a49abbd7d71494abfcd3a","placeholder":"​","style":"IPY_MODEL_3461360099d04e28bdd629aef46ba70c","value":" 4/15 [10:32&lt;22:55, 125.02s/it]"}},"6148fb4a47414152837bc4946d2acf64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85e465e2f76f4eaeb34c411dfbc0ccc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bbc3e0bf8214e379b5d621b9d5884f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf27d162387641b887094c671290c844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed3a12fecc5f4b8eba58844c7c64e694":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"692b876d136a49abbd7d71494abfcd3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3461360099d04e28bdd629aef46ba70c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install necessary dependencies\n","!pip install torch torchvision tqdm\n","\n","# Import libraries\n","import os\n","import random\n","import shutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision.models import resnet50, resnet101, resnet152\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from torchvision.models.resnet import ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n","\n","# Set random seed for reproducibility\n","random.seed(42)\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Gsduq01-mYS","executionInfo":{"status":"ok","timestamp":1687192901645,"user_tz":-120,"elapsed":28590,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}},"outputId":"8d60bb2a-6765-49bc-e1cb-209e997e48f1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fc718296590>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Define variables\n","model_name = \"egoist_V1\"\n","train_percentage = 0.8\n","batch_size = 64\n","num_epochs = 30\n","load_pretrained = False  # Change this to True if you want to load a pretrained model\n","pretrained_model_name = \"egoist_V1_c14_m101\"  # Specify the name of the pretrained model\n","model_type = \"101\"\n","\n","# Define paths\n","project_folder = '/content/drive/MyDrive/colab_projects/ai_image_grouper'\n","original_folder = f'{project_folder}/data'\n","training_folder = f'{project_folder}/training'\n","validation_folder = f'{project_folder}/validation'\n","model_folder = f'{project_folder}/models'"],"metadata":{"id":"IjNudfD5-qM2","executionInfo":{"status":"ok","timestamp":1687192903973,"user_tz":-120,"elapsed":2,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define functions\n","\n","def count_folders(directory):\n","    return len([folder for folder in os.listdir(directory) if os.path.isdir(os.path.join(directory, folder))])\n","\n","def remove_folder(directory):\n","    if os.path.exists(directory):\n","        shutil.rmtree(directory)\n","\n","num_categories = count_folders(original_folder)"],"metadata":{"id":"knk_E_gB-sxN","executionInfo":{"status":"ok","timestamp":1687192907146,"user_tz":-120,"elapsed":628,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["remove_folder(training_folder)\n","remove_folder(validation_folder)\n","\n","# Load and preprocess data\n","\n","print(\"\\nConverting PNG images to RGBA\")\n","# Recursively search for PNG images and convert them to RGBA\n","for root, dirs, files in os.walk(original_folder):\n","    for file in files:\n","        if file.endswith('.png'):\n","            image_path = os.path.join(root, file)\n","            im = Image.open(image_path)\n","            if im.format == 'PNG' and im.mode != 'RGBA':\n","                im = im.convert('RGBA')\n","                im.save(image_path)\n","\n","print(\"Conversion completed!\\n\")\n","\n","print(f\"Categories found: {num_categories}\\n\")\n","\n","# Create the \"training\" and \"validation\" folders if they don't exist\n","os.makedirs(training_folder, exist_ok=True)\n","os.makedirs(validation_folder, exist_ok=True)\n","os.makedirs(model_folder, exist_ok=True)\n","\n","# Randomly split images into training and validation folders\n","for root, dirs, files in os.walk(original_folder):\n","    # Get the relative path from the original folder\n","    relative_path = os.path.relpath(root, original_folder)\n","\n","    # Create the corresponding folders in the \"training\" and \"validation\" directories\n","    training_dir = os.path.join(training_folder, relative_path)\n","    validation_dir = os.path.join(validation_folder, relative_path)\n","    os.makedirs(training_dir, exist_ok=True)\n","    os.makedirs(validation_dir, exist_ok=True)\n","\n","    # Randomly shuffle the list of files\n","    random.shuffle(files)\n","\n","    # Split the files based on the train_percentage\n","    train_size = int(len(files) * train_percentage)\n","    train_files = files[:train_size]\n","    validation_files = files[train_size:]\n","\n","    # Move the files to the \"training\" and \"validation\" folders\n","    for file in train_files:\n","        src = os.path.join(root, file)\n","        dst = os.path.join(training_dir, file)\n","        shutil.copy(src, dst)\n","\n","    for file in validation_files:\n","        src = os.path.join(root, file)\n","        dst = os.path.join(validation_dir, file)\n","        shutil.copy(src, dst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDmQjx7S-0dH","executionInfo":{"status":"ok","timestamp":1687192426645,"user_tz":-120,"elapsed":88936,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}},"outputId":"571e2ee1-b676-4925-8c43-f0a8157fdcfd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Converting PNG images to RGBA\n","Conversion completed!\n","\n","Categories found: 14\n","\n"]}]},{"cell_type":"code","source":["# Define the model\n","\n","if model_type == '50':\n","    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n","elif model_type == '101':\n","    model = resnet101(weights=ResNet101_Weights.DEFAULT)\n","elif model_type == '152':\n","    model = resnet152(weights=ResNet152_Weights.DEFAULT)\n","else:\n","    raise ValueError(\"Invalid model_type. Please choose from '50', '101', or '152'.\")\n","\n","num_features = model.fc.in_features\n","\n","# Modify the fully connected layer for the number of categories\n","model.fc = nn.Linear(num_features, num_categories)"],"metadata":{"id":"LNDwN5e4_oAI","executionInfo":{"status":"ok","timestamp":1687192920269,"user_tz":-120,"elapsed":8952,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cea595fa-81fa-4ae6-bc01-923a9ee48b58"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n","100%|██████████| 171M/171M [00:02<00:00, 70.0MB/s]\n"]}]},{"cell_type":"code","source":["# Define data transformations\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(),\n","    transforms.RandomRotation(degrees=45),\n","    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"dbXN2tXE_qkJ","executionInfo":{"status":"ok","timestamp":1687192925184,"user_tz":-120,"elapsed":313,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Create data loaders\n","\n","# Create the ImageFolder dataset for training\n","training_dataset = ImageFolder(training_folder, transform=train_transform)\n","\n","# Create a data loader for the training dataset\n","training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Create the ImageFolder dataset for validation\n","validation_dataset = ImageFolder(validation_folder, transform=val_transform)\n","\n","# Create a data loader for the validation dataset\n","validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"6Ndb1Wur_tCT","executionInfo":{"status":"ok","timestamp":1687192930679,"user_tz":-120,"elapsed":2878,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Define loss function, optimizer, and scheduler\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)"],"metadata":{"id":"z8v9yNsN_vTZ","executionInfo":{"status":"ok","timestamp":1687192933008,"user_tz":-120,"elapsed":237,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","\n","# Check if a pretrained model should be loaded\n","epoch_index = 0\n","\n","if load_pretrained:\n","    checkpoint = torch.load(f\"{model_folder}/{pretrained_model_name}.ckp\")\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    num_epochs += checkpoint['epoch']\n","    epoch_index = checkpoint['epoch']\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")\n","model.to(device)\n","\n","best_val_accuracy = 0.0\n","\n","print(\"Training...\\n\")\n","\n","for epoch in range(epoch_index, num_epochs):\n","    model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","\n","    progress_bar = tqdm(enumerate(training_loader), total=len(training_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n","\n","    for batch_idx, (images, labels) in progress_bar:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(training_loader)}\")\n","\n","    # Validation loop\n","    model.eval()  # Set the model to evaluation mode\n","\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    val_progress_bar = tqdm(validation_loader, desc=f\"Epoch {epoch+1}/{num_epochs}, Validation\", leave=False)\n","\n","    with torch.no_grad():\n","        for val_images, val_labels in val_progress_bar:\n","            val_images, val_labels = val_images.to(device), val_labels.to(device)\n","            val_outputs = model(val_images)\n","            val_batch_loss = criterion(val_outputs, val_labels)\n","            val_loss += val_batch_loss.item()\n","\n","            _, val_predicted = torch.max(val_outputs.data, 1)\n","            val_total += val_labels.size(0)\n","            val_correct += (val_predicted == val_labels).sum().item()\n","\n","            val_progress_bar.set_postfix(loss=val_loss / len(validation_loader), accuracy=(val_correct / val_total) * 100)\n","\n","    val_accuracy = (val_correct / val_total) * 100\n","    val_loss /= len(validation_loader)\n","    tqdm.write(f\"Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","    torch.save({\n","        'epoch': epoch+1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': val_loss,\n","    }, f'{model_folder}/{model_name}_c{num_categories}_m{model_type}.ckp')\n","\n","    # Check if the current model has the best validation accuracy\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        # Save the current best model\n","        torch.save(model.state_dict(), f'{model_folder}/{model_name}_c{num_categories}_m{model_type}.pth')\n","\n","    # Update the learning rate\n","    scheduler.step(val_loss)\n","\n","print(f\"\\nTraining completed. Best validation accuracy: {best_val_accuracy:.4f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464,"referenced_widgets":["06dff03f04f44623aaa0d90f8fc7b094","abff852a6d664145b0ade5623e3c2429","39702af6e3954da4a058d5bdd93b9459","b0197fabd1334a90bf5a53efcc7ff943","6148fb4a47414152837bc4946d2acf64","85e465e2f76f4eaeb34c411dfbc0ccc3","3bbc3e0bf8214e379b5d621b9d5884f8","bf27d162387641b887094c671290c844","ed3a12fecc5f4b8eba58844c7c64e694","692b876d136a49abbd7d71494abfcd3a","3461360099d04e28bdd629aef46ba70c"]},"id":"6LQt_Q8r_xTB","outputId":"46ba7e3d-6ffc-46b5-996c-d2d3deca38a9","executionInfo":{"status":"error","timestamp":1687193583071,"user_tz":-120,"elapsed":634209,"user":{"displayName":"Andrea Antonutti","userId":"09644378419550615025"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu\n","Training...\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 1/30:   0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06dff03f04f44623aaa0d90f8fc7b094"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c47f80224cf1>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}